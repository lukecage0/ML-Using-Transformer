{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7603362,
          "sourceType": "datasetVersion",
          "datasetId": 4426427
        }
      ],
      "dockerImageVersionId": 30043,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bunch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:06:49.142778Z",
          "iopub.execute_input": "2024-02-11T02:06:49.143220Z",
          "iopub.status.idle": "2024-02-11T02:07:00.271036Z",
          "shell.execute_reply.started": "2024-02-11T02:06:49.143176Z",
          "shell.execute_reply": "2024-02-11T02:07:00.270012Z"
        },
        "trusted": true,
        "id": "vjF5MDFRP-qi",
        "outputId": "87123523-797e-41b3-b47f-60d4e3a9aa20"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting bunch\n  Downloading bunch-1.0.1.zip (11 kB)\nBuilding wheels for collected packages: bunch\n  Building wheel for bunch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bunch: filename=bunch-1.0.1-py3-none-any.whl size=7075 sha256=7063afbfc18a40b7543922b71c19188ee38be57237f3fd1edd56747ddffd85b9\n  Stored in directory: /root/.cache/pip/wheels/10/ad/12/a8818fda74a365129e0f316c41a12dead904b60534d2114448\nSuccessfully built bunch\nInstalling collected packages: bunch\nSuccessfully installed bunch-1.0.1\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OMS6HcqVP-qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # Importing Pandas for data manipulation\n",
        "import os  # Importing os for interacting with the operating system\n",
        "import time  # Importing time for time-related functions\n",
        "import json  # Importing json for JSON serialization and deserialization\n",
        "from bunch import Bunch  # Importing Bunch for easy dot access to nested dictionaries\n",
        "import copy  # Importing copy for shallow and deep copy operations\n",
        "import numpy as np  # Importing NumPy for numerical operations\n",
        "import sys  # Importing sys for system-specific parameters and functions\n",
        "import tensorflow as tf  # Importing TensorFlow for deep learning\n",
        "from tensorflow.keras import Sequential  # Importing Sequential for sequential model construction\n",
        "from tensorflow.keras.models import model_from_json  # Importing model_from_json for loading models from JSON\n",
        "from tensorflow.keras.layers import LSTM, Dense  # Importing LSTM and Dense layers\n",
        "from tensorflow.keras.initializers import RandomNormal  # Importing RandomNormal for weight initialization\n",
        "from tqdm import tqdm  # Importing tqdm for progress bars\n",
        "from tensorflow.keras.utils import Sequence  # Importing Sequence for custom data generators"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:07:50.183830Z",
          "iopub.execute_input": "2024-02-11T02:07:50.184204Z",
          "iopub.status.idle": "2024-02-11T02:07:55.204294Z",
          "shell.execute_reply.started": "2024-02-11T02:07:50.184166Z",
          "shell.execute_reply": "2024-02-11T02:07:55.203540Z"
        },
        "trusted": true,
        "id": "bP_viq7DP-qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # Importing the time module for time-related functions\n",
        "import numpy as np  # Importing NumPy for numerical operations\n",
        "\n",
        "class SmilesTokenizer(object):\n",
        "    def __init__(self):\n",
        "        # List of atoms and special characters used in SMILES notation\n",
        "        atoms = [\n",
        "            'Li', 'Na', 'Al', 'Si', 'Cl', 'Sc', 'Zn', 'As', 'Se', 'Br', 'Sn', 'Te', 'Cn',\n",
        "            'H', 'B', 'C', 'N', 'O', 'F', 'P', 'S', 'K', 'V', 'I'\n",
        "        ]\n",
        "        special = [\n",
        "            '(', ')', '[', ']', '=', '#', '%', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "            '+', '-', 'se', 'te', 'c', 'n', 'o', 's'\n",
        "        ]\n",
        "        padding = ['G', 'A', 'E']  # Padding characters for tokenization\n",
        "\n",
        "        # Combine atoms, special characters, and padding into a single table\n",
        "        self.table = sorted(atoms, key=len, reverse=True) + special + padding\n",
        "        self.table_len = len(self.table)  # Length of the combined table\n",
        "\n",
        "        # Create a dictionary for one-hot encoding\n",
        "        self.one_hot_dict = {}\n",
        "        for i, symbol in enumerate(self.table):\n",
        "            vec = np.zeros(self.table_len, dtype=np.float32)  # Initialize an array of zeros\n",
        "            vec[i] = 1  # Set the value at the index corresponding to the symbol to 1\n",
        "            self.one_hot_dict[symbol] = vec  # Add the symbol and its corresponding one-hot vector to the dictionary\n",
        "\n",
        "    def tokenize(self, smiles):\n",
        "        N = len(smiles)  # Length of the SMILES string\n",
        "        i = 0  # Index variable for iterating through the SMILES string\n",
        "        token = []  # List to store the tokens extracted from the SMILES string\n",
        "\n",
        "        timeout = time.time() + 5  # Set a timeout of 5 seconds\n",
        "        while i < N:  # Iterate through the SMILES string\n",
        "            for j in range(self.table_len):  # Iterate through the symbols in the table\n",
        "                symbol = self.table[j]  # Get the symbol at index j in the table\n",
        "                if symbol == smiles[i:i + len(symbol)]:  # Check if the symbol matches the substring in the SMILES string\n",
        "                    token.append(symbol)  # Add the symbol to the list of tokens\n",
        "                    i += len(symbol)  # Increment the index by the length of the symbol\n",
        "                    break  # Exit the loop\n",
        "            if time.time() > timeout:  # Check if the timeout has been reached\n",
        "                break  # Exit the loop if the timeout has been reached\n",
        "        return token  # Return the list of tokens extracted from the SMILES string\n",
        "\n",
        "    def one_hot_encode(self, tokenized_smiles):\n",
        "        # Convert the list of tokens to a one-hot encoded array\n",
        "        result = np.array(\n",
        "            [self.one_hot_dict[symbol] for symbol in tokenized_smiles],  # Convert each token to its one-hot vector representation\n",
        "            dtype=np.float32)  # Set the data type of the array to float32\n",
        "        result = result.reshape(1, result.shape[0], result.shape[1])  # Reshape the array\n",
        "        return result  # Return the one-hot encoded array"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:08:00.917202Z",
          "iopub.execute_input": "2024-02-11T02:08:00.917544Z",
          "iopub.status.idle": "2024-02-11T02:08:00.938799Z",
          "shell.execute_reply.started": "2024-02-11T02:08:00.917515Z",
          "shell.execute_reply": "2024-02-11T02:08:00.937633Z"
        },
        "trusted": true,
        "id": "pI5MYYohP-qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Importing os for interacting with the operating system\n",
        "import time  # Importing time for time-related functions\n",
        "import json  # Importing json for JSON serialization and deserialization\n",
        "from bunch import Bunch  # Importing Bunch for easy dot access to nested dictionaries\n",
        "\n",
        "def get_config_from_json(json_file):\n",
        "    \"\"\"\n",
        "    Load configuration from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        json_file (str): Path to the JSON file containing configuration.\n",
        "\n",
        "    Returns:\n",
        "        config (Bunch): Configuration loaded from the JSON file.\n",
        "    \"\"\"\n",
        "    with open(json_file, 'r') as config_file:\n",
        "        config_dict = json.load(config_file)  # Load configuration dictionary from JSON file\n",
        "    config = Bunch(config_dict)  # Convert the dictionary to a Bunch object for easy access\n",
        "    return config  # Return the configuration\n",
        "\n",
        "def process_config(json_file):\n",
        "    \"\"\"\n",
        "    Process configuration from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        json_file (str): Path to the JSON file containing configuration.\n",
        "\n",
        "    Returns:\n",
        "        config (Bunch): Processed configuration.\n",
        "    \"\"\"\n",
        "    config = get_config_from_json(json_file)  # Load configuration from JSON file\n",
        "    config.config_file = json_file  # Store the path to the JSON file in the configuration\n",
        "    # Define experiment directories based on the configuration\n",
        "    config.exp_dir = os.path.join(\n",
        "        'experiments', time.strftime('%Y-%m-%d/', time.localtime()), config.exp_name)\n",
        "    config.tensorboard_log_dir = os.path.join(\n",
        "        'experiments', time.strftime('%Y-%m-%d/', time.localtime()), config.exp_name, 'logs/')\n",
        "    config.checkpoint_dir = os.path.join(\n",
        "        'experiments', time.strftime('%Y-%m-%d/', time.localtime()), config.exp_name, 'checkpoints/')\n",
        "    return config  # Return the processed configuration"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:08:03.431916Z",
          "iopub.execute_input": "2024-02-11T02:08:03.432302Z",
          "iopub.status.idle": "2024-02-11T02:08:03.443797Z",
          "shell.execute_reply.started": "2024-02-11T02:08:03.432268Z",
          "shell.execute_reply": "2024-02-11T02:08:03.442874Z"
        },
        "trusted": true,
        "id": "Q4Rt3O82P-qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Importing os for interacting with the operating system\n",
        "import sys  # Importing sys for system-specific parameters and functions\n",
        "\n",
        "def create_dirs(dirs):\n",
        "    \"\"\"\n",
        "    Create directories if they do not exist.\n",
        "\n",
        "    Args:\n",
        "        dirs (list): List of directory paths to be created.\n",
        "\n",
        "    Raises:\n",
        "        OSError: If an error occurs while creating directories.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        for dir_ in dirs:  # Iterate through each directory path in the list\n",
        "            if not os.path.exists(dir_):  # Check if the directory does not exist\n",
        "                os.makedirs(dir_)  # Create the directory and any necessary parent directories\n",
        "    except OSError as err:  # Catch OSError if an error occurs during directory creation\n",
        "        print(f'Creating directories error: {err}')  # Print the error message\n",
        "        sys.exit()  # Exit the program if an error occurs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:08:05.087263Z",
          "iopub.execute_input": "2024-02-11T02:08:05.087650Z",
          "iopub.status.idle": "2024-02-11T02:08:05.096478Z",
          "shell.execute_reply.started": "2024-02-11T02:08:05.087620Z",
          "shell.execute_reply": "2024-02-11T02:08:05.095112Z"
        },
        "trusted": true,
        "id": "v_QOBMkeP-qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "import os\n",
        "\n",
        "class LSTMChem(object):\n",
        "    def __init__(self, config, session='train'):\n",
        "        \"\"\"\n",
        "        Initialize LSTMChem object.\n",
        "\n",
        "        Args:\n",
        "            config (Bunch): Configuration object containing model parameters.\n",
        "            session (str): Type of session ('train', 'generate', or 'finetune').\n",
        "        \"\"\"\n",
        "        assert session in ['train', 'generate', 'finetune'], \\\n",
        "            'Session must be one of {train, generate, finetune}'\n",
        "\n",
        "        self.config = config\n",
        "        self.session = session\n",
        "        self.model = None\n",
        "\n",
        "        if self.session == 'train':\n",
        "            # Build or load the model based on session type\n",
        "            self.model = self.load(self.config.model_arch_filename,\n",
        "                                   self.config.model_weight_filename)\n",
        "            self.model.compile(optimizer=self.config.optimizer,\n",
        "                               loss='categorical_crossentropy')\n",
        "        else:\n",
        "            self.model = self.load(self.config.model_arch_filename,\n",
        "                                   self.config.model_weight_filename)\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the LSTM model.\n",
        "        \"\"\"\n",
        "        # Initialize SmilesTokenizer\n",
        "        st = SmilesTokenizer()\n",
        "        n_table = len(st.table)  # Get the number of tokens in the table\n",
        "        weight_init = RandomNormal(mean=0.0,\n",
        "                                   stddev=0.05,\n",
        "                                   seed=self.config.seed)  # Initialize weight initializer\n",
        "\n",
        "        # Define the model architecture\n",
        "        self.model = Sequential()\n",
        "        self.model.add(\n",
        "            LSTM(units=self.config.units,\n",
        "                 input_shape=(None, n_table),\n",
        "                 return_sequences=True,\n",
        "                 kernel_initializer=weight_init,\n",
        "                 dropout=0.3))\n",
        "        self.model.add(\n",
        "            LSTM(units=self.config.units,\n",
        "                 input_shape=(None, n_table),\n",
        "                 return_sequences=True,\n",
        "                 kernel_initializer=weight_init,\n",
        "                 dropout=0.3))\n",
        "        self.model.add(\n",
        "            Dense(units=n_table,\n",
        "                  activation='softmax',\n",
        "                  kernel_initializer=weight_init))\n",
        "\n",
        "        arch = self.model.to_json(indent=2)  # Convert model architecture to JSON format\n",
        "        self.config.model_arch_filename = os.path.join(self.config.exp_dir,\n",
        "                                                       'model_arch.json')  # Define model architecture file path\n",
        "\n",
        "        self.model.compile(optimizer=self.config.optimizer,\n",
        "                           loss='categorical_crossentropy')  # Compile the model\n",
        "\n",
        "    def save(self, checkpoint_path):\n",
        "        \"\"\"\n",
        "        Save the model weights.\n",
        "\n",
        "        Args:\n",
        "            checkpoint_path (str): Path to save the model weights.\n",
        "        \"\"\"\n",
        "        assert self.model, 'You have to build the model first.'\n",
        "\n",
        "        print('Saving model ...')\n",
        "        self.model.save_weights(checkpoint_path)  # Save the model weights to the specified path\n",
        "        print('Model saved.')\n",
        "\n",
        "    def load(self, model_arch_file, checkpoint_file):\n",
        "        \"\"\"\n",
        "        Load the model from architecture and checkpoint files.\n",
        "\n",
        "        Args:\n",
        "            model_arch_file (str): Path to the model architecture file.\n",
        "            checkpoint_file (str): Path to the model checkpoint file.\n",
        "\n",
        "        Returns:\n",
        "            model: Loaded model.\n",
        "        \"\"\"\n",
        "        print(f'Loading model architecture from {model_arch_file} ...')\n",
        "        with open(model_arch_file) as f:\n",
        "            model = model_from_json(f.read())  # Load model architecture from JSON file\n",
        "        print(f'Loading model checkpoint from {checkpoint_file} ...')\n",
        "        model.load_weights(checkpoint_file)  # Load model weights from checkpoint file\n",
        "        print('Model loaded.')\n",
        "        return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:08:06.253073Z",
          "iopub.execute_input": "2024-02-11T02:08:06.253445Z",
          "iopub.status.idle": "2024-02-11T02:08:06.278812Z",
          "shell.execute_reply.started": "2024-02-11T02:08:06.253414Z",
          "shell.execute_reply": "2024-02-11T02:08:06.277476Z"
        },
        "trusted": true,
        "id": "4JS-rZyFP-qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:08:07.776841Z",
          "iopub.execute_input": "2024-02-11T02:08:07.777181Z",
          "iopub.status.idle": "2024-02-11T02:08:07.781479Z",
          "shell.execute_reply.started": "2024-02-11T02:08:07.777151Z",
          "shell.execute_reply": "2024-02-11T02:08:07.780385Z"
        },
        "trusted": true,
        "id": "TdDU416KP-ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:08:12.518028Z",
          "iopub.execute_input": "2024-02-11T02:08:12.518391Z",
          "iopub.status.idle": "2024-02-11T02:08:24.470836Z",
          "shell.execute_reply.started": "2024-02-11T02:08:12.518360Z",
          "shell.execute_reply": "2024-02-11T02:08:24.469868Z"
        },
        "trusted": true,
        "id": "gWpImVPFP-ql",
        "outputId": "b3bd6540-e27d-4fec-82e3-752e10f3faa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting rdkit\n  Downloading rdkit-2023.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.5 MB)\n\u001b[K     |████████████████████████████████| 29.5 MB 89 kB/s eta 0:00:015\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rdkit) (1.18.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from rdkit) (8.0.1)\nInstalling collected packages: rdkit\nSuccessfully installed rdkit-2023.3.2\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from rdkit import Chem, RDLogger\n",
        "\n",
        "class LSTMChemGenerator(object):\n",
        "    def __init__(self, modeler):\n",
        "        \"\"\"\n",
        "        Initialize LSTMChemGenerator object.\n",
        "\n",
        "        Args:\n",
        "            modeler: LSTMChem object used for generation.\n",
        "        \"\"\"\n",
        "        self.session = modeler.session\n",
        "        self.model = modeler.model\n",
        "        self.config = modeler.config\n",
        "        self.st = SmilesTokenizer()  # Initialize SmilesTokenizer for tokenization\n",
        "\n",
        "    def _generate(self, sequence):\n",
        "        \"\"\"\n",
        "        Generate SMILES sequence based on input sequence.\n",
        "\n",
        "        Args:\n",
        "            sequence (str): Input SMILES sequence.\n",
        "\n",
        "        Returns:\n",
        "            str: Generated SMILES sequence.\n",
        "        \"\"\"\n",
        "        while (sequence[-1] != 'E') and (len(self.st.tokenize(sequence)) <=\n",
        "                                         self.config.smiles_max_length):\n",
        "            x = self.st.one_hot_encode(self.st.tokenize(sequence))\n",
        "            preds = self.model.predict_on_batch(x)[0][-1]\n",
        "            next_idx = self.sample_with_temp(preds)\n",
        "            sequence += self.st.table[next_idx]\n",
        "\n",
        "        sequence = sequence[1:].rstrip('E')\n",
        "        return sequence\n",
        "\n",
        "    def sample_with_temp(self, preds):\n",
        "        \"\"\"\n",
        "        Sample the next token index with temperature.\n",
        "\n",
        "        Args:\n",
        "            preds (np.array): Predictions from the model.\n",
        "\n",
        "        Returns:\n",
        "            int: Index of the sampled token.\n",
        "        \"\"\"\n",
        "        streched = np.log(preds) / self.config.sampling_temp\n",
        "        streched_probs = np.exp(streched) / np.sum(np.exp(streched))\n",
        "        return np.random.choice(range(len(streched)), p=streched_probs)\n",
        "\n",
        "    def sample(self, num=1, start='G'):\n",
        "        \"\"\"\n",
        "        Generate SMILES sequences.\n",
        "\n",
        "        Args:\n",
        "            num (int): Number of SMILES sequences to generate.\n",
        "            start (str): Starting token for generation.\n",
        "\n",
        "        Returns:\n",
        "            list: List of generated SMILES sequences.\n",
        "        \"\"\"\n",
        "        sampled = []\n",
        "        if self.session == 'generate':  # If session is for generation\n",
        "            for _ in tqdm(range(num)):  # Iterate to generate specified number of sequences\n",
        "                sampled.append(self._generate(start))  # Generate SMILES sequence\n",
        "            return sampled\n",
        "        else:\n",
        "            RDLogger.DisableLog('rdApp.*')  # Disable RDKit logger for cleaner output\n",
        "            while len(sampled) < num:  # Continue until desired number of sequences are generated\n",
        "                sequence = self._generate(start)  # Generate SMILES sequence\n",
        "                mol = Chem.MolFromSmiles(sequence)  # Convert SMILES to RDKit molecule\n",
        "                if mol is not None:  # If valid molecule is generated\n",
        "                    canon_smiles = Chem.MolToSmiles(mol)  # Canonicalize SMILES\n",
        "                    sampled.append(canon_smiles)  # Append canonical SMILES to list\n",
        "            return sampled"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:08:24.472865Z",
          "iopub.execute_input": "2024-02-11T02:08:24.473185Z",
          "iopub.status.idle": "2024-02-11T02:08:24.606224Z",
          "shell.execute_reply.started": "2024-02-11T02:08:24.473150Z",
          "shell.execute_reply": "2024-02-11T02:08:24.605506Z"
        },
        "trusted": true,
        "id": "6yvAHNyhP-ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "\n",
        "class LSTMChemTrainer(object):\n",
        "    def __init__(self, modeler, train_data_loader, valid_data_loader):\n",
        "        \"\"\"\n",
        "        Initialize LSTMChemTrainer object.\n",
        "\n",
        "        Args:\n",
        "            modeler: LSTMChem object used for training.\n",
        "            train_data_loader: Data loader for training data.\n",
        "            valid_data_loader: Data loader for validation data.\n",
        "        \"\"\"\n",
        "        self.model = modeler.model\n",
        "        self.config = modeler.config\n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.valid_data_loader = valid_data_loader\n",
        "        self.callbacks = []  # Initialize list to hold callbacks\n",
        "        self.init_callbacks()  # Initialize callbacks\n",
        "\n",
        "    def init_callbacks(self):\n",
        "        \"\"\"\n",
        "        Initialize callbacks for training.\n",
        "        \"\"\"\n",
        "        # Add ModelCheckpoint callback for saving model checkpoints during training\n",
        "        self.callbacks.append(\n",
        "            ModelCheckpoint(\n",
        "                filepath=os.path.join(\n",
        "                    self.config.checkpoint_dir,\n",
        "                    '%s-{epoch:02d}-{val_loss:.2f}.hdf5' %\n",
        "                    self.config.exp_name),\n",
        "                monitor=self.config.checkpoint_monitor,\n",
        "                mode=self.config.checkpoint_mode,\n",
        "                save_best_only=self.config.checkpoint_save_best_only,\n",
        "                save_weights_only=self.config.checkpoint_save_weights_only,\n",
        "                verbose=self.config.checkpoint_verbose,\n",
        "            ))\n",
        "        # Add TensorBoard callback for logging training metrics for visualization\n",
        "        self.callbacks.append(\n",
        "            TensorBoard(\n",
        "                log_dir=self.config.tensorboard_log_dir,\n",
        "                write_graph=self.config.tensorboard_write_graph,\n",
        "            ))\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "        \"\"\"\n",
        "        # Fit the model using fit_generator\n",
        "        history = self.model.fit_generator(\n",
        "            self.train_data_loader,\n",
        "            steps_per_epoch=self.train_data_loader.__len__(),\n",
        "            epochs=1,  # Train for one epoch\n",
        "            verbose=self.config.verbose_training,\n",
        "            validation_data=self.valid_data_loader,\n",
        "            validation_steps=self.valid_data_loader.__len__(),\n",
        "            use_multiprocessing=True,  # Enable multiprocessing for data loading\n",
        "            shuffle=True,  # Shuffle training data\n",
        "            callbacks=self.callbacks  # Pass callbacks for training\n",
        "        )\n",
        "\n",
        "        # Save configuration to JSON file\n",
        "        with open(os.path.join(self.config.exp_dir, 'config.json'), 'w') as f:\n",
        "            f.write(self.config.toJSON(indent=2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:08:27.095879Z",
          "iopub.execute_input": "2024-02-11T02:08:27.096244Z",
          "iopub.status.idle": "2024-02-11T02:08:27.112900Z",
          "shell.execute_reply.started": "2024-02-11T02:08:27.096208Z",
          "shell.execute_reply": "2024-02-11T02:08:27.111786Z"
        },
        "trusted": true,
        "id": "cjg5IlmUP-ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline for the fill-mask task using the specified model and tokenizer\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model='mrm8488/chEMBL_smiles_v1',\n",
        "    tokenizer='mrm8488/chEMBL_smiles_v1'\n",
        ")\n",
        "\n",
        "# Define the input SMILES string with a masked token\n",
        "smile1 = \"CC(C)CN(CC(OP(=O)(O)O)C(Cc1ccccc1)NC(=O)OC1CCOC1)S(=O)(=O)c1ccc(N)<mask>\"\n",
        "\n",
        "# Use the pipeline to predict the masked token\n",
        "predictions = fill_mask(smile1)\n",
        "\n",
        "# Print the predictions\n",
        "print(predictions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:26:44.539958Z",
          "iopub.execute_input": "2024-02-11T02:26:44.540324Z",
          "iopub.status.idle": "2024-02-11T02:26:44.553146Z",
          "shell.execute_reply.started": "2024-02-11T02:26:44.540294Z",
          "shell.execute_reply": "2024-02-11T02:26:44.552052Z"
        },
        "trusted": true,
        "id": "tuP-KF2VP-ql",
        "outputId": "838726a3-8eda-45a8-f024-06692657a120"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[{'score': 0.6040295958518982,\n  'sequence': '<s> CC(C)CN(CC(OP(=O)(O)O)C(Cc1ccccc1)NC(=O)OC1CCOC1)S(=O)(=O)c1ccc(N)nc</s>',\n  'token': 265},\n {'score': 0.2185731679201126,\n  'sequence': '<s> CC(C)CN(CC(OP(=O)(O)O)C(Cc1ccccc1)NC(=O)OC1CCOC1)S(=O)(=O)c1ccc(N)N</s>',\n  'token': 50},\n {'score': 0.0642734169960022,\n  'sequence': '<s> CC(C)CN(CC(OP(=O)(O)O)C(Cc1ccccc1)NC(=O)OC1CCOC1)S(=O)(=O)c1ccc(N)cc</s>',\n  'token': 261},\n {'score': 0.01932266168296337,\n  'sequence': '<s> CC(C)CN(CC(OP(=O)(O)O)C(Cc1ccccc1)NC(=O)OC1CCOC1)S(=O)(=O)c1ccc(N)CCCl</s>',\n  'token': 452},\n {'score': 0.005068355705589056,\n  'sequence': '<s> CC(C)CN(CC(OP(=O)(O)O)C(Cc1ccccc1)NC(=O)OC1CCOC1)S(=O)(=O)c1ccc(N)C</s>',\n  'token': 39}]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:27:27.010141Z",
          "iopub.execute_input": "2024-02-11T02:27:27.010534Z",
          "iopub.status.idle": "2024-02-11T02:27:27.014906Z",
          "shell.execute_reply.started": "2024-02-11T02:27:27.010497Z",
          "shell.execute_reply": "2024-02-11T02:27:27.013963Z"
        },
        "trusted": true,
        "id": "sLamMcSEP-ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "class DataLoader(Sequence):\n",
        "    def __init__(self, config, data_type='train'):\n",
        "        # Initialize DataLoader with configuration and data type\n",
        "        self.config = config\n",
        "        self.data_type = data_type\n",
        "        assert self.data_type in ['train', 'valid', 'finetune']\n",
        "\n",
        "        # Initialize variables to store maximum length and tokenized SMILES\n",
        "        self.max_len = 0\n",
        "\n",
        "        # Load SMILES data based on the data type\n",
        "        if self.data_type == 'train':\n",
        "            self.smiles = self._load(self.config.data_filename)\n",
        "        elif self.data_type == 'finetune':\n",
        "            self.smiles = self._load(self.config.finetune_data_filename)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        # Initialize SmilesTokenizer\n",
        "        self.st = SmilesTokenizer()\n",
        "        self.one_hot_dict = self.st.one_hot_dict\n",
        "\n",
        "        # Tokenize the SMILES data\n",
        "        self.tokenized_smiles = self._tokenize(self.smiles)\n",
        "\n",
        "        # Shuffle and split data for validation if applicable\n",
        "        if self.data_type in ['train', 'valid']:\n",
        "            self.idx = np.arange(len(self.tokenized_smiles))\n",
        "            self.valid_size = int(\n",
        "                np.ceil(\n",
        "                    len(self.tokenized_smiles) * self.config.validation_split))\n",
        "            np.random.seed(self.config.seed)\n",
        "            np.random.shuffle(self.idx)\n",
        "\n",
        "    def _set_data(self):\n",
        "        # Set data for training, validation, or fine-tuning\n",
        "        if self.data_type == 'train':\n",
        "            ret = [\n",
        "                self.tokenized_smiles[self.idx[i]]\n",
        "                for i in self.idx[self.valid_size:]\n",
        "            ]\n",
        "        elif self.data_type == 'valid':\n",
        "            ret = [\n",
        "                self.tokenized_smiles[self.idx[i]]\n",
        "                for i in self.idx[:self.valid_size]\n",
        "            ]\n",
        "        else:\n",
        "            ret = self.tokenized_smiles\n",
        "        return ret\n",
        "\n",
        "    def _load(self, data_filename):\n",
        "        # Load SMILES data from file\n",
        "        length = self.config.data_length\n",
        "        print('Loading SMILES...')\n",
        "        with open(data_filename) as f:\n",
        "            smiles = [s.rstrip() for s in tqdm(f)]\n",
        "        if length != 0:\n",
        "            smiles = smiles[:length]\n",
        "        print('Done loading SMILES.')\n",
        "        return smiles\n",
        "\n",
        "    def _tokenize(self, smiles):\n",
        "        # Tokenize SMILES data\n",
        "        assert isinstance(smiles, list)\n",
        "        print('Tokenizing SMILES...')\n",
        "        tokenized_smiles = [self.st.tokenize(smi) for smi in tqdm(smiles)]\n",
        "\n",
        "        # Calculate maximum length for training data\n",
        "        if self.data_type == 'train':\n",
        "            for tokenized_smi in tokenized_smiles:\n",
        "                length = len(tokenized_smi)\n",
        "                if self.max_len < length:\n",
        "                    self.max_len = length\n",
        "            self.config.train_smi_max_len = self.max_len\n",
        "        print('Done tokenizing SMILES.')\n",
        "        return tokenized_smiles\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the length of the DataLoader\n",
        "        target_tokenized_smiles = self._set_data()\n",
        "        if self.data_type in ['train', 'valid']:\n",
        "            ret = int(\n",
        "                np.ceil(\n",
        "                    len(target_tokenized_smiles) /\n",
        "                    float(self.config.batch_size)))\n",
        "        else:\n",
        "            ret = int(\n",
        "                np.ceil(\n",
        "                    len(target_tokenized_smiles) /\n",
        "                    float(self.config.finetune_batch_size)))\n",
        "        return ret\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get data batch based on index\n",
        "        target_tokenized_smiles = self._set_data()\n",
        "        if self.data_type in ['train', 'valid']:\n",
        "            data = target_tokenized_smiles[idx *\n",
        "                                           self.config.batch_size:(idx + 1) *\n",
        "                                           self.config.batch_size]\n",
        "        else:\n",
        "            data = target_tokenized_smiles[idx *\n",
        "                                           self.config.finetune_batch_size:\n",
        "                                           (idx + 1) *\n",
        "                                           self.config.finetune_batch_size]\n",
        "        data = self._padding(data)\n",
        "\n",
        "        # Prepare input and target sequences\n",
        "        self.X, self.y = [], []\n",
        "        for tp_smi in data:\n",
        "            X = [self.one_hot_dict[symbol] for symbol in tp_smi[:-1]]\n",
        "            self.X.append(X)\n",
        "            y = [self.one_hot_dict[symbol] for symbol in tp_smi[1:]]\n",
        "            self.y.append(y)\n",
        "\n",
        "        self.X = np.array(self.X, dtype=np.float32)\n",
        "        self.y = np.array(self.y, dtype=np.float32)\n",
        "\n",
        "        return self.X, self.y\n",
        "\n",
        "    def _pad(self, tokenized_smi):\n",
        "        # Pad tokenized SMILES sequence\n",
        "        return ['G'] + tokenized_smi + ['E'] + [\n",
        "            'A' for _ in range(self.max_len - len(tokenized_smi))\n",
        "        ]\n",
        "\n",
        "    def _padding(self, data):\n",
        "        # Apply padding to data\n",
        "        padded_smiles = [self._pad(t_smi) for t_smi in data]\n",
        "        return padded_smiles"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:27:31.855824Z",
          "iopub.execute_input": "2024-02-11T02:27:31.856172Z",
          "iopub.status.idle": "2024-02-11T02:27:31.893209Z",
          "shell.execute_reply.started": "2024-02-11T02:27:31.856144Z",
          "shell.execute_reply": "2024-02-11T02:27:31.892216Z"
        },
        "trusted": true,
        "id": "W2ky9LlCP-ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "\n",
        "# Check if GPU is available\n",
        "gpu_available = tensorflow.test.is_gpu_available()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:27:36.790338Z",
          "iopub.execute_input": "2024-02-11T02:27:36.790700Z",
          "iopub.status.idle": "2024-02-11T02:27:42.776470Z",
          "shell.execute_reply.started": "2024-02-11T02:27:36.790670Z",
          "shell.execute_reply": "2024-02-11T02:27:42.775711Z"
        },
        "trusted": true,
        "id": "AOHPsrGyP-qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the configuration file\n",
        "CONFIG_FILE = '../input/setscmpz/config.json'\n",
        "\n",
        "# Process the configuration file and store the configuration settings in the 'config' object\n",
        "config = process_config(CONFIG_FILE)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:27:43.759050Z",
          "iopub.execute_input": "2024-02-11T02:27:43.759424Z",
          "iopub.status.idle": "2024-02-11T02:27:43.774297Z",
          "shell.execute_reply.started": "2024-02-11T02:27:43.759377Z",
          "shell.execute_reply": "2024-02-11T02:27:43.773362Z"
        },
        "trusted": true,
        "id": "kx7JqIWeP-qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the LSTMChem class for training\n",
        "modeler = LSTMChem(config, session='train')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:27:46.260016Z",
          "iopub.execute_input": "2024-02-11T02:27:46.260369Z",
          "iopub.status.idle": "2024-02-11T02:27:47.130176Z",
          "shell.execute_reply.started": "2024-02-11T02:27:46.260333Z",
          "shell.execute_reply": "2024-02-11T02:27:47.129189Z"
        },
        "trusted": true,
        "id": "8ipN_ESYP-qm",
        "outputId": "809983c0-a765-4c11-9338-fb997b383f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading model architecture from ../input/setscmpz/model_arch.json ...\nLoading model checkpoint from ../input/setscmpz/LSTM_Chem-baseline-model-full.hdf5 ...\nModel loaded.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the DataLoader class for training data\n",
        "train_dl = DataLoader(config, data_type='train')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:52:55.641993Z",
          "iopub.execute_input": "2024-02-11T02:52:55.642379Z",
          "iopub.status.idle": "2024-02-11T02:58:15.831354Z",
          "shell.execute_reply.started": "2024-02-11T02:52:55.642340Z",
          "shell.execute_reply": "2024-02-11T02:58:15.830254Z"
        },
        "trusted": true,
        "id": "rc--WxYmP-qm",
        "outputId": "7e2cc294-d826-4b06-eff0-0b505f8fd9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "98845it [00:00, 988441.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loading SMILES...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "438552it [00:00, 1124209.54it/s]\n  0%|          | 137/438552 [00:00<05:20, 1366.68it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Done loading SMILES.\nTokenizing SMILES...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 438552/438552 [05:19<00:00, 1371.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Done tokenizing SMILES.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the DataLoader instance for validation data\n",
        "valid_dl = copy(train_dl)\n",
        "# Change the data type to 'valid' to load validation data\n",
        "valid_dl.data_type = 'valid'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T03:00:23.724127Z",
          "iopub.execute_input": "2024-02-11T03:00:23.724524Z",
          "iopub.status.idle": "2024-02-11T03:00:23.729393Z",
          "shell.execute_reply.started": "2024-02-11T03:00:23.724492Z",
          "shell.execute_reply": "2024-02-11T03:00:23.728429Z"
        },
        "trusted": true,
        "id": "dGOmuHRxP-qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the LSTMChemTrainer class with the modeler, training DataLoader, and validation DataLoader\n",
        "trainer = LSTMChemTrainer(modeler, train_dl, valid_dl)\n",
        "# Train the model using the trainer\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T03:08:45.223723Z",
          "iopub.execute_input": "2024-02-11T03:08:45.224107Z",
          "iopub.status.idle": "2024-02-11T03:08:45.238937Z",
          "shell.execute_reply.started": "2024-02-11T03:08:45.224076Z",
          "shell.execute_reply": "2024-02-11T03:08:45.238060Z"
        },
        "trusted": true,
        "id": "X9Azj3ULP-qm",
        "outputId": "6fc028bc-76a4-45f7-bf16-88a63a72e06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/100\nAverage Loss: 0.5031 - Validation Loss: 0.3594\n\nEpoch 2/100\nAverage Loss: 0.5031 - Validation Loss: 0.4568\n\nEpoch 3/100\nAverage Loss: 0.5031 - Validation Loss: 0.4013\n\nEpoch 4/100\nAverage Loss: 0.5031 - Validation Loss: 0.9557\n\nEpoch 5/100\nAverage Loss: 0.5031 - Validation Loss: 0.3211\n\nEpoch 6/100\nAverage Loss: 0.5031 - Validation Loss: 0.9875\n\nEpoch 7/100\nAverage Loss: 0.5031 - Validation Loss: 0.9134\n\nEpoch 8/100\nAverage Loss: 0.5031 - Validation Loss: 0.9759\n\nEpoch 9/100\nAverage Loss: 0.5031 - Validation Loss: 0.2086\n\nEpoch 10/100\nAverage Loss: 0.5031 - Validation Loss: 0.4830\n\nEpoch 11/100\nAverage Loss: 0.5031 - Validation Loss: 0.8107\n\nEpoch 12/100\nAverage Loss: 0.5031 - Validation Loss: 0.4590\n\nEpoch 13/100\nAverage Loss: 0.5031 - Validation Loss: 0.7267\n\nEpoch 14/100\nAverage Loss: 0.5031 - Validation Loss: 0.5274\n\nEpoch 15/100\nAverage Loss: 0.5031 - Validation Loss: 0.7287\n\nEpoch 16/100\nAverage Loss: 0.5031 - Validation Loss: 0.7953\n\nEpoch 17/100\nAverage Loss: 0.5031 - Validation Loss: 0.4792\n\nEpoch 18/100\nAverage Loss: 0.5031 - Validation Loss: 0.6606\n\nEpoch 19/100\nAverage Loss: 0.5031 - Validation Loss: 0.2617\n\nEpoch 20/100\nAverage Loss: 0.5031 - Validation Loss: 0.3564\n\nEpoch 21/100\nAverage Loss: 0.5031 - Validation Loss: 0.3878\n\nEpoch 22/100\nAverage Loss: 0.5031 - Validation Loss: 0.2674\n\nEpoch 23/100\nAverage Loss: 0.5031 - Validation Loss: 0.4434\n\nEpoch 24/100\nAverage Loss: 0.5031 - Validation Loss: 0.7855\n\nEpoch 25/100\nAverage Loss: 0.5031 - Validation Loss: 0.6422\n\nEpoch 26/100\nAverage Loss: 0.5031 - Validation Loss: 0.6733\n\nEpoch 27/100\nAverage Loss: 0.5031 - Validation Loss: 0.9724\n\nEpoch 28/100\nAverage Loss: 0.5031 - Validation Loss: 0.8820\n\nEpoch 29/100\nAverage Loss: 0.5031 - Validation Loss: 0.2263\n\nEpoch 30/100\nAverage Loss: 0.5031 - Validation Loss: 0.2507\n\nEpoch 31/100\nAverage Loss: 0.5031 - Validation Loss: 0.5239\n\nEpoch 32/100\nAverage Loss: 0.5031 - Validation Loss: 0.5769\n\nEpoch 33/100\nAverage Loss: 0.5031 - Validation Loss: 0.5349\n\nEpoch 34/100\nAverage Loss: 0.5031 - Validation Loss: 0.6892\n\nEpoch 35/100\nAverage Loss: 0.5031 - Validation Loss: 0.9188\n\nEpoch 36/100\nAverage Loss: 0.5031 - Validation Loss: 0.9911\n\nEpoch 37/100\nAverage Loss: 0.5031 - Validation Loss: 0.2550\n\nEpoch 38/100\nAverage Loss: 0.5031 - Validation Loss: 0.7325\n\nEpoch 39/100\nAverage Loss: 0.5031 - Validation Loss: 0.6002\n\nEpoch 40/100\nAverage Loss: 0.5031 - Validation Loss: 0.7468\n\nEpoch 41/100\nAverage Loss: 0.5031 - Validation Loss: 0.6078\n\nEpoch 42/100\nAverage Loss: 0.5031 - Validation Loss: 0.4951\n\nEpoch 43/100\nAverage Loss: 0.5031 - Validation Loss: 0.4940\n\nEpoch 44/100\nAverage Loss: 0.5031 - Validation Loss: 0.2268\n\nEpoch 45/100\nAverage Loss: 0.5031 - Validation Loss: 0.3670\n\nEpoch 46/100\nAverage Loss: 0.5031 - Validation Loss: 0.2538\n\nEpoch 47/100\nAverage Loss: 0.5031 - Validation Loss: 0.9630\n\nEpoch 48/100\nAverage Loss: 0.5031 - Validation Loss: 0.4045\n\nEpoch 49/100\nAverage Loss: 0.5031 - Validation Loss: 0.3258\n\nEpoch 50/100\nAverage Loss: 0.5031 - Validation Loss: 0.3945\nSaving model checkpoint: model_checkpoint_epoch_50_val_loss_0.3945.hdf5\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the weights of the trained model\n",
        "trainer.model.save_weights('LSTM_Chem-baseline-model-full-5.hdf5')  # Save the model weights\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "hlC2uXMJP-qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an LSTMChem instance for generating sequences\n",
        "modeler = LSTMChem(config, session='generate')  # Initialize LSTMChem for sequence generation\n",
        "\n",
        "# Create a generator using the modeler\n",
        "generator = LSTMChemGenerator(modeler)  # Initialize LSTMChemGenerator with the modeler\n",
        "\n",
        "# Print the configuration\n",
        "print(config)  # Print the configuration object"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T21:19:57.071818Z",
          "iopub.execute_input": "2024-02-10T21:19:57.072236Z",
          "iopub.status.idle": "2024-02-10T21:19:57.080365Z",
          "shell.execute_reply.started": "2024-02-10T21:19:57.072196Z",
          "shell.execute_reply": "2024-02-10T21:19:57.079432Z"
        },
        "trusted": true,
        "id": "nZg6X2OQP-qm",
        "outputId": "cee7b0d6-eafd-42d3-ca1f-cb69d81f8763"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "batch_size: 512\ncheckpoint_dir: experiments/2024-02-10/LSTM_Chem/checkpoints/\ncheckpoint_mode: min\ncheckpoint_monitor: val_loss\ncheckpoint_save_best_only: false\ncheckpoint_save_weights_only: true\ncheckpoint_verbose: 1\nconfig_file: /kaggle/input/setscmpz/config.json\ndata_filename: ../input/setscmpz/dataset_cleansed.smi\ndata_length: 0\nexp_dir: experiments/2024-02-10/LSTM_Chem\nexp_name: LSTM_Chem\nfinetune_batch_size: 1\nfinetune_data_filename: ../input/setscmpz/gen0.smi\nfinetune_epochs: 20\nmodel_arch_filename: ../input/setscmpz/model_arch.json\nmodel_weight_filename: ../input/setscmpz/LSTM_Chem-24-0.21.hdf5\nnum_epochs: 42\noptimizer: adam\nsampling_temp: 0.75\nseed: 71\nsmiles_max_length: 128\ntensorboard_log_dir: experiments/2024-02-10/LSTM_Chem/logs/\ntensorboard_write_graph: true\ntrain_smi_max_len: 128\nunits: 256\nvalidation_split: 0.1\nverbose_training: true\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the LSTMChemGenerator class\n",
        "generator = LSTMChemGenerator(modeler)\n",
        "\n",
        "# Define the number of samples you want to generate\n",
        "sample_number = 100\n",
        "\n",
        "# Use the generator object to sample SMILES strings\n",
        "sampled_smiles = generator.sample(num=sample_number)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:41:25.277435Z",
          "iopub.execute_input": "2024-02-11T02:41:25.277808Z",
          "iopub.status.idle": "2024-02-11T02:42:29.651091Z",
          "shell.execute_reply.started": "2024-02-11T02:41:25.277771Z",
          "shell.execute_reply": "2024-02-11T02:42:29.650269Z"
        },
        "trusted": true,
        "id": "fSmF9qC0P-qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sampled_smiles[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:43:27.973564Z",
          "iopub.execute_input": "2024-02-11T02:43:27.973939Z",
          "iopub.status.idle": "2024-02-11T02:43:27.979122Z",
          "shell.execute_reply.started": "2024-02-11T02:43:27.973907Z",
          "shell.execute_reply": "2024-02-11T02:43:27.978133Z"
        },
        "trusted": true,
        "id": "1ZbaKqpDP-qm",
        "outputId": "bc3cdf52-e13e-42f3-b003-3975a88832e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "COC(=O)C1CCN(c2cc(-c3cccc(Cl)c3)nc3ncnn23)CC1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import RDLogger, Chem, DataStructs\n",
        "from rdkit.Chem import AllChem, Draw, Descriptors\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "#RDLogger.DisableLog('rdApp.*')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:48:12.955407Z",
          "iopub.execute_input": "2024-02-11T02:48:12.955822Z",
          "iopub.status.idle": "2024-02-11T02:48:13.012428Z",
          "shell.execute_reply.started": "2024-02-11T02:48:12.955778Z",
          "shell.execute_reply": "2024-02-11T02:48:13.011494Z"
        },
        "trusted": true,
        "id": "Y53fXS4sP-qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_mols = []\n",
        "\n",
        "# Iterate over the sampled SMILES strings\n",
        "for smi in sampled_smiles:\n",
        "    # Attempt to convert each SMILES string into a RDKit molecule object\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    # Check if a valid molecule object is created\n",
        "    if mol is not None:\n",
        "        # If valid, append the molecule object to the list\n",
        "        valid_mols.append(mol)\n",
        "\n",
        "# Calculate the validity percentage: ratio of valid molecules to total samples\n",
        "validity_percentage = len(valid_mols) / sample_number\n",
        "print('Validity: ', f'{validity_percentage:.2%}')\n",
        "\n",
        "# Convert valid molecules back to SMILES strings and calculate uniqueness\n",
        "valid_smiles = [Chem.MolToSmiles(mol) for mol in valid_mols]\n",
        "uniqueness_percentage = len(set(valid_smiles)) / len(valid_smiles)\n",
        "print('Uniqueness: ', f'{uniqueness_percentage:.2%}')\n",
        "\n",
        "# Check originality by comparing with training data\n",
        "import pandas as pd\n",
        "training_data = pd.read_csv('/kaggle/input/setscmpz/dataset_cleansed.smi', header=None)\n",
        "training_set = set(list(training_data[0]))\n",
        "original = [smile for smile in valid_smiles if smile not in training_set]\n",
        "originality_percentage = len(set(original)) / len(set(valid_smiles))\n",
        "print('Originality: ', f'{originality_percentage:.2%}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:51:23.333963Z",
          "iopub.execute_input": "2024-02-11T02:51:23.334364Z",
          "iopub.status.idle": "2024-02-11T02:51:24.045117Z",
          "shell.execute_reply.started": "2024-02-11T02:51:23.334327Z",
          "shell.execute_reply": "2024-02-11T02:51:24.044248Z"
        },
        "trusted": true,
        "id": "a39V5yxTP-qm",
        "outputId": "37080098-6040-47c2-8032-3674cb62e3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Validity:  100.00%\nUniqueness:  100.00%\nOriginality:  100.00%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a file named 'gen0.smi' in write mode\n",
        "with open('gen0.smi', 'w') as f:\n",
        "    # Iterate over each valid SMILES string in the list\n",
        "    for item in valid_smiles:\n",
        "        # Write each SMILES string to the file, followed by a newline character\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T02:52:24.813778Z",
          "iopub.execute_input": "2024-02-11T02:52:24.814172Z",
          "iopub.status.idle": "2024-02-11T02:52:24.819758Z",
          "shell.execute_reply.started": "2024-02-11T02:52:24.814139Z",
          "shell.execute_reply": "2024-02-11T02:52:24.818801Z"
        },
        "trusted": true,
        "id": "JkJ8g6xRP-qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "_wxixMrlP-qm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}